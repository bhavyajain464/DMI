{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_svdd_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fa02227adaaa4e0e83d8e0b865972fee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9a9964f228744c9d9a903270c982717c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c1b9f29f61de4612bbb69d866ffb9d66",
              "IPY_MODEL_6e62aa880543468d95cedc1cb265cf19"
            ]
          }
        },
        "9a9964f228744c9d9a903270c982717c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c1b9f29f61de4612bbb69d866ffb9d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_845e5c074f914823ba359cbb739e0335",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9912422,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9912422,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_91ee9b404d4c484db965d0cd46963e75"
          }
        },
        "6e62aa880543468d95cedc1cb265cf19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4e60920c40bc4169980e63c9b54e4339",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9913344/? [07:10&lt;00:00, 23022.80it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2e958ae732c7462aa1b9bfa91195ecbf"
          }
        },
        "845e5c074f914823ba359cbb739e0335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "91ee9b404d4c484db965d0cd46963e75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e60920c40bc4169980e63c9b54e4339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2e958ae732c7462aa1b9bfa91195ecbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b9c24ce259744bb92037ccbd4bcea0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3ae6887392b04ac08e779bcf91e953f1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0fc299bc98d74902a0d31ee4ca25e801",
              "IPY_MODEL_b97d879a432f4d87878f96509e72c268"
            ]
          }
        },
        "3ae6887392b04ac08e779bcf91e953f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0fc299bc98d74902a0d31ee4ca25e801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2de5c776e9e24d4092c20e7e0e2db6bf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9b40e5b9bd154ff2aab0fe689f0eebfb"
          }
        },
        "b97d879a432f4d87878f96509e72c268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5256adaefcc545998d2a2ce086b5e5b4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29696/? [00:52&lt;00:00, 564.63it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_24e0508d8bfc4c75bf517ab00590fcb6"
          }
        },
        "2de5c776e9e24d4092c20e7e0e2db6bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9b40e5b9bd154ff2aab0fe689f0eebfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5256adaefcc545998d2a2ce086b5e5b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "24e0508d8bfc4c75bf517ab00590fcb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88870de7ce3a4891a9704b43c407fa24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_255d06d2dcdb42278b0fb5bab2c0f029",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ab1f3260a0354cd8b8612171d4406397",
              "IPY_MODEL_2f02fa2ec1d84a4fa092bfdf3b886929"
            ]
          }
        },
        "255d06d2dcdb42278b0fb5bab2c0f029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab1f3260a0354cd8b8612171d4406397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_64e9691b70a149d3b45f6c02edb2db11",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1648877,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1648877,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc7e562a3c134e65afab1a09119e2a73"
          }
        },
        "2f02fa2ec1d84a4fa092bfdf3b886929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ee477df576f9444b9f70111e8db0defb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1649664/? [01:15&lt;00:00, 21910.76it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0dadd71b24a94638ba78f74725151726"
          }
        },
        "64e9691b70a149d3b45f6c02edb2db11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc7e562a3c134e65afab1a09119e2a73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee477df576f9444b9f70111e8db0defb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0dadd71b24a94638ba78f74725151726": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c1e54ad78f0640ee89e197931b3da770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bbd24790c050487b94fa646721778732",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bfbf5e908d64456c823fe154d2dc7540",
              "IPY_MODEL_7e8ee3559ca54046bff822e5ef2cea30"
            ]
          }
        },
        "bbd24790c050487b94fa646721778732": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bfbf5e908d64456c823fe154d2dc7540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_77156b3c22a04998ab212aaa9681abd8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4542,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4542,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3b7fef585684497e81c8856c00fc8f3e"
          }
        },
        "7e8ee3559ca54046bff822e5ef2cea30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_af4039781fed4f59ad6271580615e879",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5120/? [00:00&lt;00:00, 13446.14it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cd116f8acc79464d94f55d4947855ff9"
          }
        },
        "77156b3c22a04998ab212aaa9681abd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3b7fef585684497e81c8856c00fc8f3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af4039781fed4f59ad6271580615e879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cd116f8acc79464d94f55d4947855ff9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqPHD5X_T1rN"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJZVwumauMYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f3c6aba-4c5e-44f8-f135-4eb6e8065fe2"
      },
      "source": [
        "!pip install barbar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting barbar\n",
            "  Downloading https://files.pythonhosted.org/packages/48/1f/9b69ce144f484cfa00feb09fa752139658961de6303ea592487738d0b53c/barbar-0.2.1-py3-none-any.whl\n",
            "Installing collected packages: barbar\n",
            "Successfully installed barbar-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp7mVwRWumAs"
      },
      "source": [
        "from torch import optim\n",
        "from barbar import Bar\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pickle"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU_eWbeDtpnE"
      },
      "source": [
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1 and classname != 'Conv':\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"Linear\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "\n",
        "def global_contrast_normalization(x):\n",
        "    mean = torch.mean(x)\n",
        "    x -= mean\n",
        "    x_scale = torch.mean(torch.abs(x))\n",
        "    x /= x_scale\n",
        "    return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WZ9rz-JT1d5"
      },
      "source": [
        "class network(nn.Module):\n",
        "    def __init__(self, z_dim=32):\n",
        "        super(network, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "        self.fc1 = nn.Linear(4 * 7 * 7, z_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn1(x)))\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc1(x)\n",
        "\n",
        "\n",
        "class autoencoder(nn.Module):\n",
        "    def __init__(self, z_dim=32):\n",
        "        super(autoencoder, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "        self.fc1 = nn.Linear(4 * 7 * 7, z_dim, bias=False)\n",
        "\n",
        "        self.deconv1 = nn.ConvTranspose2d(2, 4, 5, bias=False, padding=2)\n",
        "        self.bn3 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "        self.deconv2 = nn.ConvTranspose2d(4, 8, 5, bias=False, padding=3)\n",
        "        self.bn4 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "        self.deconv3 = nn.ConvTranspose2d(8, 1, 5, bias=False, padding=2)\n",
        "        \n",
        "    def encode(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn1(x)))\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc1(x)\n",
        "   \n",
        "    def decode(self, x):\n",
        "        x = x.view(x.size(0), int(self.z_dim / 16), 4, 4)\n",
        "        x = F.interpolate(F.leaky_relu(x), scale_factor=2)\n",
        "        x = self.deconv1(x)\n",
        "        x = F.interpolate(F.leaky_relu(self.bn3(x)), scale_factor=2)\n",
        "        x = self.deconv2(x)\n",
        "        x = F.interpolate(F.leaky_relu(self.bn4(x)), scale_factor=2)\n",
        "        x = self.deconv3(x)\n",
        "        return torch.sigmoid(x)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        x_hat = self.decode(z)\n",
        "        return x_hat"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESPc7wIBT1ZP"
      },
      "source": [
        "class MNIST_loader(data.Dataset):\n",
        "    def __init__(self, data, target, transform):\n",
        "        self.data = data\n",
        "        self.target = target\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        if self.transform:\n",
        "            x = Image.fromarray(x.numpy(), mode='L')\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def get_mnist(args, data_dir='./data/mnist/'):\n",
        "    min_max = [(-0.8826567065619495, 9.001545489292527),\n",
        "                (-0.6661464580883915, 20.108062262467364),\n",
        "                (-0.7820454743183202, 11.665100841080346),\n",
        "                (-0.7645772083211267, 12.895051191467457),\n",
        "                (-0.7253923114302238, 12.683235701611533),\n",
        "                (-0.7698501867861425, 13.103278415430502),\n",
        "                (-0.778418217980696, 10.457837397569108),\n",
        "                (-0.7129780970522351, 12.057777597673047),\n",
        "                (-0.8280402650205075, 10.581538445782988),\n",
        "                (-0.7369959242164307, 10.697039838804978)]\n",
        "\n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Lambda(lambda x: global_contrast_normalization(x)),\n",
        "                                    transforms.Normalize([min_max[args.normal_class][0]],\n",
        "                                                         [min_max[args.normal_class][1] \\\n",
        "                                                         -min_max[args.normal_class][0]])])\n",
        "    train = datasets.MNIST(root=data_dir, train=True, download=True)\n",
        "    test = datasets.MNIST(root=data_dir, train=False, download=True)\n",
        "\n",
        "    x_train = train.data\n",
        "    y_train = train.targets\n",
        "\n",
        "    x_train = x_train[np.where(y_train==args.normal_class)]\n",
        "    y_train = y_train[np.where(y_train==args.normal_class)]\n",
        "                                    \n",
        "    data_train = MNIST_loader(x_train, y_train, transform)\n",
        "    dataloader_train = DataLoader(data_train, batch_size=args.batch_size, \n",
        "                                  shuffle=True, num_workers=0)\n",
        "    \n",
        "    x_test = test.data\n",
        "    y_test = test.targets\n",
        "    y_test = np.where(y_test==args.normal_class, 0, 1)\n",
        "    data_test = MNIST_loader(x_test, y_test, transform)\n",
        "    dataloader_test = DataLoader(data_test, batch_size=args.batch_size, \n",
        "                                  shuffle=True, num_workers=0)\n",
        "    return dataloader_train, dataloader_test"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "633jY4jPw75s"
      },
      "source": [
        "def get_mnist_after(args, training_data, training_label, testing_data, testing_label):\n",
        "  min_max = [(-0.8826567065619495, 9.001545489292527),\n",
        "                (-0.6661464580883915, 20.108062262467364),\n",
        "                (-0.7820454743183202, 11.665100841080346),\n",
        "                (-0.7645772083211267, 12.895051191467457),\n",
        "                (-0.7253923114302238, 12.683235701611533),\n",
        "                (-0.7698501867861425, 13.103278415430502),\n",
        "                (-0.778418217980696, 10.457837397569108),\n",
        "                (-0.7129780970522351, 12.057777597673047),\n",
        "                (-0.8280402650205075, 10.581538445782988),\n",
        "                (-0.7369959242164307, 10.697039838804978)]\n",
        "\n",
        "  transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                  transforms.Lambda(lambda x: global_contrast_normalization(x)),\n",
        "                                  transforms.Normalize([min_max[args.normal_class][0]],\n",
        "                                                        [min_max[args.normal_class][1] \\\n",
        "                                                        -min_max[args.normal_class][0]])])\n",
        "  data_train = MNIST_loader(training_data, training_label, transform)\n",
        "  dataloader_train = DataLoader(data_train, batch_size=args.batch_size, \n",
        "                                shuffle=True, num_workers=0)\n",
        "  data_test = MNIST_loader(testing_data, testing_label, transform)\n",
        "  dataloader_test = DataLoader(data_test, batch_size=args.batch_size, \n",
        "                                shuffle=True, num_workers=0)\n",
        "  return dataloader_train, dataloader_test"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQsSuZxaVWJx"
      },
      "source": [
        "class TrainerDeepSVDD:\n",
        "    def __init__(self, args, data, device):\n",
        "        self.args = args\n",
        "        self.train_loader, self.test_loader = data\n",
        "        self.device = device\n",
        "    \n",
        "\n",
        "    def pretrain(self):\n",
        "        ae = autoencoder(self.args.latent_dim).to(self.device)\n",
        "        ae.apply(weights_init_normal)\n",
        "        optimizer = optim.Adam(ae.parameters(), lr=self.args.lr_ae,\n",
        "                               weight_decay=self.args.weight_decay_ae)\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
        "                    milestones=self.args.lr_milestones, gamma=0.1)\n",
        "        \n",
        "        ae.train()\n",
        "        for epoch in range(self.args.num_epochs_ae):\n",
        "            total_loss = 0\n",
        "            for x, _ in Bar(self.train_loader):\n",
        "                x = x.float().to(self.device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                x_hat = ae(x)\n",
        "                reconst_loss = torch.mean(torch.sum((x_hat - x) ** 2, dim=tuple(range(1, x_hat.dim()))))\n",
        "                reconst_loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                total_loss += reconst_loss.item()\n",
        "            scheduler.step()\n",
        "            print('Pretraining Autoencoder... Epoch: {}, Loss: {:.3f}'.format(\n",
        "                   epoch, total_loss/len(self.train_loader)))\n",
        "        self.save_weights_for_DeepSVDD(ae, self.train_loader) \n",
        "    \n",
        "\n",
        "    def save_weights_for_DeepSVDD(self, model, dataloader):\n",
        "        c = self.set_c(model, dataloader)\n",
        "        net = network(self.args.latent_dim).to(self.device)\n",
        "        state_dict = model.state_dict()\n",
        "        net.load_state_dict(state_dict, strict=False)\n",
        "        torch.save({'center': c.cpu().data.numpy().tolist(),\n",
        "                    'net_dict': net.state_dict()}, './pretrained_parameters.pth')\n",
        "    \n",
        "\n",
        "    def set_c(self, model, dataloader, eps=0.1):\n",
        "        model.eval()\n",
        "        z_ = []\n",
        "        with torch.no_grad():\n",
        "            for x, _ in dataloader:\n",
        "                x = x.float().to(self.device)\n",
        "                z = model.encode(x)\n",
        "                z_.append(z.detach())\n",
        "        z_ = torch.cat(z_)\n",
        "        c = torch.mean(z_, dim=0)\n",
        "        c[(abs(c) < eps) & (c < 0)] = -eps\n",
        "        c[(abs(c) < eps) & (c > 0)] = eps\n",
        "        return c\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        net = network().to(self.device)\n",
        "        \n",
        "        if self.args.pretrain==True:\n",
        "            state_dict = torch.load('./pretrained_parameters.pth')\n",
        "            net.load_state_dict(state_dict['net_dict'])\n",
        "            c = torch.Tensor(state_dict['center']).to(self.device)\n",
        "        else:\n",
        "            net.apply(weights_init_normal)\n",
        "            c = torch.randn(self.args.latent_dim).to(self.device)\n",
        "        \n",
        "        optimizer = optim.Adam(net.parameters(), lr=self.args.lr,\n",
        "                               weight_decay=self.args.weight_decay)\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
        "                    milestones=self.args.lr_milestones, gamma=0.1)\n",
        "\n",
        "        net.train()\n",
        "        for epoch in range(self.args.num_epochs):\n",
        "            total_loss = 0\n",
        "            for x, _ in Bar(self.train_loader):\n",
        "                x = x.float().to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                z = net(x)\n",
        "                loss = torch.mean(torch.sum((z - c) ** 2, dim=1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "            scheduler.step()\n",
        "            print('Training Deep SVDD... Epoch: {}, Loss: {:.3f}'.format(\n",
        "                   epoch, total_loss/len(self.train_loader)))\n",
        "        self.net = net\n",
        "        self.c = c"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx-KsJ6-VzQl"
      },
      "source": [
        "def eval(net, c, dataloader, device):\n",
        "    scores = []\n",
        "    labels = []\n",
        "    net.eval()\n",
        "    print('Testing...')\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.float().to(device)\n",
        "            z = net(x)\n",
        "            score = torch.sum((z - c) ** 2, dim=1)\n",
        "\n",
        "            scores.append(score.detach().cpu())\n",
        "            labels.append(y.cpu())\n",
        "    labels, scores = torch.cat(labels).numpy(), torch.cat(scores).numpy()\n",
        "    print('ROC AUC score: {:.2f}'.format(roc_auc_score(labels, scores)*100))\n",
        "    return labels, scores"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fa02227adaaa4e0e83d8e0b865972fee",
            "9a9964f228744c9d9a903270c982717c",
            "c1b9f29f61de4612bbb69d866ffb9d66",
            "6e62aa880543468d95cedc1cb265cf19",
            "845e5c074f914823ba359cbb739e0335",
            "91ee9b404d4c484db965d0cd46963e75",
            "4e60920c40bc4169980e63c9b54e4339",
            "2e958ae732c7462aa1b9bfa91195ecbf",
            "6b9c24ce259744bb92037ccbd4bcea0c",
            "3ae6887392b04ac08e779bcf91e953f1",
            "0fc299bc98d74902a0d31ee4ca25e801",
            "b97d879a432f4d87878f96509e72c268",
            "2de5c776e9e24d4092c20e7e0e2db6bf",
            "9b40e5b9bd154ff2aab0fe689f0eebfb",
            "5256adaefcc545998d2a2ce086b5e5b4",
            "24e0508d8bfc4c75bf517ab00590fcb6",
            "88870de7ce3a4891a9704b43c407fa24",
            "255d06d2dcdb42278b0fb5bab2c0f029",
            "ab1f3260a0354cd8b8612171d4406397",
            "2f02fa2ec1d84a4fa092bfdf3b886929",
            "64e9691b70a149d3b45f6c02edb2db11",
            "bc7e562a3c134e65afab1a09119e2a73",
            "ee477df576f9444b9f70111e8db0defb",
            "0dadd71b24a94638ba78f74725151726",
            "c1e54ad78f0640ee89e197931b3da770",
            "bbd24790c050487b94fa646721778732",
            "bfbf5e908d64456c823fe154d2dc7540",
            "7e8ee3559ca54046bff822e5ef2cea30",
            "77156b3c22a04998ab212aaa9681abd8",
            "3b7fef585684497e81c8856c00fc8f3e",
            "af4039781fed4f59ad6271580615e879",
            "cd116f8acc79464d94f55d4947855ff9"
          ]
        },
        "id": "vQaNgF-xaWg0",
        "outputId": "65ec5703-a1be-486f-bacc-05cd49827d3e"
      },
      "source": [
        "class Args:\n",
        "\n",
        "    num_epochs=150\n",
        "    num_epochs_ae=150\n",
        "    patience=50\n",
        "    lr=1e-4\n",
        "    weight_decay=0.5e-6\n",
        "    weight_decay_ae=0.5e-3\n",
        "    lr_ae=1e-4\n",
        "    lr_milestones=[50]\n",
        "    batch_size=200\n",
        "    pretrain=True\n",
        "    latent_dim=32\n",
        "    normal_class=1\n",
        "    \n",
        "    \n",
        "args = Args()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = get_mnist(args)\n",
        "\n",
        "deep_SVDD = TrainerDeepSVDD(args, data, device)\n",
        "\n",
        "if args.pretrain:\n",
        "    deep_SVDD.pretrain()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa02227adaaa4e0e83d8e0b865972fee",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b9c24ce259744bb92037ccbd4bcea0c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88870de7ce3a4891a9704b43c407fa24",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1e54ad78f0640ee89e197931b3da770",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "Processing...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done!\n",
            "6742/6742: [===============================>] - ETA 0.4s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 150.321\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 106.592\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 76.484\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 56.228\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 42.788\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 33.788\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 27.496\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 22.919\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 19.473\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 16.853\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 14.816\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 13.179\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 11.848\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 10.749\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 9.826\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 9.040\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 8.361\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 7.767\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 7.236\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 6.762\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 20, Loss: 6.337\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 21, Loss: 5.954\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 22, Loss: 5.605\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 23, Loss: 5.292\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 24, Loss: 5.010\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 25, Loss: 4.748\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 26, Loss: 4.510\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 27, Loss: 4.292\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 28, Loss: 4.086\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 29, Loss: 3.899\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 30, Loss: 3.728\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 31, Loss: 3.571\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 32, Loss: 3.427\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 33, Loss: 3.296\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 34, Loss: 3.170\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 35, Loss: 3.051\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 36, Loss: 2.940\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 37, Loss: 2.840\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 38, Loss: 2.746\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 39, Loss: 2.660\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 40, Loss: 2.577\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 41, Loss: 2.501\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 42, Loss: 2.428\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 43, Loss: 2.358\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 44, Loss: 2.293\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 45, Loss: 2.230\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 46, Loss: 2.173\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 47, Loss: 2.116\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 48, Loss: 2.064\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 49, Loss: 2.012\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 50, Loss: 1.983\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 51, Loss: 1.976\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 52, Loss: 1.972\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 53, Loss: 1.966\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 54, Loss: 1.961\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 55, Loss: 1.956\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 56, Loss: 1.951\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 57, Loss: 1.945\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 58, Loss: 1.940\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 59, Loss: 1.934\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 60, Loss: 1.928\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 61, Loss: 1.923\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 62, Loss: 1.918\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 63, Loss: 1.912\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 64, Loss: 1.906\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 65, Loss: 1.901\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 66, Loss: 1.895\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 67, Loss: 1.888\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 68, Loss: 1.883\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 69, Loss: 1.877\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 70, Loss: 1.870\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 71, Loss: 1.865\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 72, Loss: 1.859\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 73, Loss: 1.853\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 74, Loss: 1.846\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 75, Loss: 1.840\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 76, Loss: 1.834\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 77, Loss: 1.828\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 78, Loss: 1.821\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 79, Loss: 1.814\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 80, Loss: 1.808\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 81, Loss: 1.801\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 82, Loss: 1.794\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 83, Loss: 1.788\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 84, Loss: 1.781\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 85, Loss: 1.775\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 86, Loss: 1.768\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 87, Loss: 1.761\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 88, Loss: 1.755\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 89, Loss: 1.747\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 90, Loss: 1.741\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 91, Loss: 1.734\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 92, Loss: 1.726\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 93, Loss: 1.719\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 94, Loss: 1.712\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 95, Loss: 1.705\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 96, Loss: 1.697\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 97, Loss: 1.690\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 98, Loss: 1.684\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 99, Loss: 1.676\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 100, Loss: 1.670\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 101, Loss: 1.661\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 102, Loss: 1.654\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 103, Loss: 1.647\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 104, Loss: 1.640\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 105, Loss: 1.633\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 106, Loss: 1.624\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 107, Loss: 1.617\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 108, Loss: 1.610\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 109, Loss: 1.603\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 110, Loss: 1.595\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 111, Loss: 1.588\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 112, Loss: 1.580\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 113, Loss: 1.574\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 114, Loss: 1.567\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 115, Loss: 1.559\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 116, Loss: 1.552\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 117, Loss: 1.544\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 118, Loss: 1.538\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 119, Loss: 1.529\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 120, Loss: 1.522\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 121, Loss: 1.516\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 122, Loss: 1.508\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 123, Loss: 1.501\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 124, Loss: 1.493\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 125, Loss: 1.486\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 126, Loss: 1.479\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 127, Loss: 1.470\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 128, Loss: 1.465\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 129, Loss: 1.456\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 130, Loss: 1.449\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 131, Loss: 1.443\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 132, Loss: 1.435\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 133, Loss: 1.428\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 134, Loss: 1.421\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 135, Loss: 1.414\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 136, Loss: 1.406\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 137, Loss: 1.399\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 138, Loss: 1.393\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 139, Loss: 1.385\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 140, Loss: 1.378\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 141, Loss: 1.372\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 142, Loss: 1.364\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 143, Loss: 1.357\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 144, Loss: 1.350\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 145, Loss: 1.343\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 146, Loss: 1.336\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 147, Loss: 1.330\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 148, Loss: 1.323\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 149, Loss: 1.316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnGZZrhPaWcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e65bd1df-13c5-42f0-f4d1-f0ba71398fcf"
      },
      "source": [
        "deep_SVDD.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 0, Loss: 0.698\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 1, Loss: 0.185\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 2, Loss: 0.060\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 3, Loss: 0.038\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 4, Loss: 0.029\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 5, Loss: 0.024\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 6, Loss: 0.020\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 7, Loss: 0.018\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 8, Loss: 0.016\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 9, Loss: 0.014\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 10, Loss: 0.013\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 11, Loss: 0.012\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 12, Loss: 0.011\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 13, Loss: 0.010\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 14, Loss: 0.009\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 15, Loss: 0.009\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 16, Loss: 0.008\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 17, Loss: 0.008\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 18, Loss: 0.007\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 19, Loss: 0.007\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 20, Loss: 0.007\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 21, Loss: 0.006\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 22, Loss: 0.006\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 23, Loss: 0.006\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 24, Loss: 0.006\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 25, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 26, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 27, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 28, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 29, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 30, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 31, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 32, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 33, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 34, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 35, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 36, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 37, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 38, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 39, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 40, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 41, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 42, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 43, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 44, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 45, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 46, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 47, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 48, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 49, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 50, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 51, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 52, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 53, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 54, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 55, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 56, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 57, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 58, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 59, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 60, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 61, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 62, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 63, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 64, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 65, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 66, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 67, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 68, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 69, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 70, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 71, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 72, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 73, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 74, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 75, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 76, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 77, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 78, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 79, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 80, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 81, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 82, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 83, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 84, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 85, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 86, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 87, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 88, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 89, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Training Deep SVDD... Epoch: 90, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 91, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 92, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 93, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 94, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 95, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 96, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 97, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 98, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 99, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 100, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 101, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 102, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 103, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 104, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 105, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 106, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 107, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 108, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 109, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 110, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 111, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 112, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 113, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 114, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 115, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 116, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 117, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 118, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 119, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 120, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 121, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 122, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 123, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 124, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 125, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 126, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 127, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 128, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 129, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 130, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 131, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 132, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 133, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 134, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 135, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 136, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 137, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 138, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 139, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 140, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 141, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 142, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 143, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 144, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 145, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 146, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 147, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 148, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 149, Loss: 0.002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdGL37S3VWDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69622e77-df9c-4a4f-a7ce-868ccd45e283"
      },
      "source": [
        "labels, scores = eval(deep_SVDD.net, deep_SVDD.c, data[1], device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "ROC AUC score: 99.26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "VL3xdJrxrmXe",
        "outputId": "ce211588-fe1b-44cd-acb4-ecf9c5903b12"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "\n",
        "scores_in = scores[np.where(labels==0)[0]]\n",
        "scores_out = scores[np.where(labels==1)[0]]\n",
        "\n",
        "\n",
        "in_ = pd.DataFrame(scores_in, columns=['Inlier'])\n",
        "out_ = pd.DataFrame(scores_out, columns=['Outlier'])\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "in_.plot.kde(ax=ax, legend=True, title='Outliers vs Inliers (Deep SVDD)')\n",
        "out_.plot.kde(ax=ax, legend=True)\n",
        "plt.xlim(-0.05, 0.08)\n",
        "ax.grid(axis='x')\n",
        "ax.grid(axis='y')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2nXZbxb0NcQ"
      },
      "source": [
        "train_loader, test_loader = data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74pDswVN_RVh"
      },
      "source": [
        "train_dataset = train_loader.dataset\n",
        "test_dataset = test_loader.dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTiqEgLCrmQ4"
      },
      "source": [
        "with open('./stored_variables.pickle', 'wb') as f:\n",
        "    pickle.dump([train_dataset.data, train_dataset.target, test_dataset.data, test_dataset.target], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_dw8AxlypQT"
      },
      "source": [
        "with open('./stored_variables.pickle', 'rb') as f:\n",
        "    training_data, training_label, testing_data, testing_label = pickle.load(f)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLgSTQbADGAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f1e1903-3047-45d4-c300-0732c5d65a2d"
      },
      "source": [
        "class Args:\n",
        "\n",
        "    num_epochs=100\n",
        "    num_epochs_ae=100\n",
        "    patience=50\n",
        "    lr=1e-4\n",
        "    weight_decay=0.5e-6\n",
        "    weight_decay_ae=0.5e-3\n",
        "    lr_ae=1e-4\n",
        "    lr_milestones=[50]\n",
        "    batch_size=200\n",
        "    pretrain=True\n",
        "    latent_dim=32\n",
        "    normal_class=1\n",
        "    \n",
        "    \n",
        "args = Args()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = get_mnist_after(args, training_data, training_label, testing_data, testing_label)\n",
        "\n",
        "deep_SVDD = TrainerDeepSVDD(args, data, device)\n",
        "\n",
        "if args.pretrain:\n",
        "    deep_SVDD.pretrain()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6742/6742: [===============================>] - ETA 0.5s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 144.626\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 104.176\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 75.124\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 54.774\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 41.280\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 32.272\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 26.098\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 21.703\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 18.437\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 15.955\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 14.012\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 12.430\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 11.151\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 10.095\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 9.206\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 8.448\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 7.791\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 7.218\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 6.691\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 6.231\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 20, Loss: 5.833\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 21, Loss: 5.479\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 22, Loss: 5.167\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 23, Loss: 4.888\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 24, Loss: 4.638\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 25, Loss: 4.410\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 26, Loss: 4.202\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 27, Loss: 4.013\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 28, Loss: 3.836\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 29, Loss: 3.675\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 30, Loss: 3.528\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 31, Loss: 3.393\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 32, Loss: 3.267\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 33, Loss: 3.152\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 34, Loss: 3.042\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 35, Loss: 2.940\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 36, Loss: 2.844\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 37, Loss: 2.755\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 38, Loss: 2.671\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 39, Loss: 2.590\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 40, Loss: 2.514\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 41, Loss: 2.442\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 42, Loss: 2.373\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 43, Loss: 2.308\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 44, Loss: 2.247\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 45, Loss: 2.189\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 46, Loss: 2.134\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 47, Loss: 2.082\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 48, Loss: 2.031\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 49, Loss: 1.985\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 50, Loss: 1.956\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 51, Loss: 1.952\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 52, Loss: 1.946\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 53, Loss: 1.942\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 54, Loss: 1.938\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 55, Loss: 1.933\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 56, Loss: 1.928\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 57, Loss: 1.922\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 58, Loss: 1.918\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 59, Loss: 1.912\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 60, Loss: 1.907\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 61, Loss: 1.901\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 62, Loss: 1.897\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 63, Loss: 1.892\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 64, Loss: 1.886\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 65, Loss: 1.881\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 66, Loss: 1.876\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 67, Loss: 1.870\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 68, Loss: 1.865\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 69, Loss: 1.858\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 70, Loss: 1.853\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 71, Loss: 1.848\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 72, Loss: 1.842\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 73, Loss: 1.836\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 74, Loss: 1.830\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 75, Loss: 1.824\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 76, Loss: 1.819\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 77, Loss: 1.812\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 78, Loss: 1.806\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 79, Loss: 1.801\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 80, Loss: 1.794\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 81, Loss: 1.788\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 82, Loss: 1.781\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 83, Loss: 1.775\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 84, Loss: 1.769\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 85, Loss: 1.762\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 86, Loss: 1.757\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 87, Loss: 1.750\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 88, Loss: 1.742\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 89, Loss: 1.736\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 90, Loss: 1.730\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 91, Loss: 1.724\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 92, Loss: 1.717\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 93, Loss: 1.710\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 94, Loss: 1.704\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 95, Loss: 1.698\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 96, Loss: 1.691\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 97, Loss: 1.685\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 98, Loss: 1.677\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 99, Loss: 1.670\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d--2YewNDF-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89c2a940-7d18-4d23-a26b-0149b5e4a0a2"
      },
      "source": [
        "deep_SVDD.train()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 0, Loss: 0.301\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 1, Loss: 0.105\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 2, Loss: 0.044\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 3, Loss: 0.028\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 4, Loss: 0.021\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 5, Loss: 0.016\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 6, Loss: 0.013\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 7, Loss: 0.011\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 8, Loss: 0.010\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 9, Loss: 0.009\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 10, Loss: 0.008\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 11, Loss: 0.007\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 12, Loss: 0.006\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 13, Loss: 0.006\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 14, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 15, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 16, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 17, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 18, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 19, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 20, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 21, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 22, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 23, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 24, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 25, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 26, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 27, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 28, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 29, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 30, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 31, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 32, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 33, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 34, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 35, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 36, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 37, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 38, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 39, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 40, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 41, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 42, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 43, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 44, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 45, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 46, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 47, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 48, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 49, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 50, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 51, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 52, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 53, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 54, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 55, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 56, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 57, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 58, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 59, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 60, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 61, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 62, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 63, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 64, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 65, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 66, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 67, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 68, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 69, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 70, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 71, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 72, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 73, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 74, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 75, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 76, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 77, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 78, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 79, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 80, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 81, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 82, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 83, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 84, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 85, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 86, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 87, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 88, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 89, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 90, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 91, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 92, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 93, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 94, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 95, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 96, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 97, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 98, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 99, Loss: 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNAUFXBD6R80",
        "outputId": "18ba57d3-93b6-4ae5-f1c5-30b4bd30be71"
      },
      "source": [
        "labels_2, scores_2 = eval(deep_SVDD.net, deep_SVDD.c, data[1], device)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "ROC AUC score: 99.45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "F05B2hhn6R09",
        "outputId": "ce08e142-1ecf-4a82-cd3f-0eb1eb80cfbd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "\n",
        "scores_in = scores_2[np.where(labels_2==0)[0]]\n",
        "scores_out = scores_2[np.where(labels_2==1)[0]]\n",
        "\n",
        "\n",
        "in_ = pd.DataFrame(scores_in, columns=['Inlier'])\n",
        "out_ = pd.DataFrame(scores_out, columns=['Outlier'])\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "in_.plot.kde(ax=ax, legend=True, title='Outliers vs Inliers (Deep SVDD)')\n",
        "out_.plot.kde(ax=ax, legend=True)\n",
        "plt.xlim(-0.05, 0.08)\n",
        "ax.grid(axis='x')\n",
        "ax.grid(axis='y')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeZCXdt_6RkB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP7uCK546Rh7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDjHdUR_6RfQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}